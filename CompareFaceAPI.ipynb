{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install opencv-python\n",
        "# !pip install matplotlib\n",
        "# !pip install Flask pyngrok opencv-python-headless matplotlib\n",
        "# !pip install tensorflow\n",
        "# !pip install --upgrade tensorflow\n",
        "# !pip install h5py\n",
        "# !pip install paddlepaddle paddleocr\n",
        "# !pip install mtcnn\n",
        "# !pip install paddleocr\n",
        "# !pip install pyngrok\n",
        "print(\"Installed all required packages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7ODpPtspJTdr"
      },
      "outputs": [],
      "source": [
        "from paddleocr import PaddleOCR\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import layers, metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from werkzeug.utils import secure_filename\n",
        "from mtcnn import MTCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM8v0SS9JZiI",
        "outputId": "a8742921-2370-461f-b5bc-8e73a8368c66"
      },
      "outputs": [],
      "source": [
        "ngrok.set_auth_token('2qwAXP2DohNlCu0ZYEgKKeZ1SON_4LRS95PBSC6gfXp39XkWQ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-ZkKgbFuWPtB"
      },
      "outputs": [],
      "source": [
        "def get_encoder(input_shape):\n",
        "    \"\"\" Returns the image encoding model \"\"\"\n",
        "\n",
        "    pretrained_model = Xception(\n",
        "        input_shape=input_shape,\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        pooling='avg',\n",
        "    )\n",
        "\n",
        "    for i in range(len(pretrained_model.layers)-27):\n",
        "        pretrained_model.layers[i].trainable = False\n",
        "\n",
        "    encode_model = Sequential([\n",
        "        pretrained_model,\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))\n",
        "    ], name=\"Encode_Model\")\n",
        "    return encode_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "sIeHau69WTut",
        "outputId": "ffacfcfe-9321-4d66-df4d-b3b6189877c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Siamese_Network\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Anchor_Input (InputLayer)      [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " Positive_Input (InputLayer)    [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " Negative_Input (InputLayer)    [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " Encode_Model (Sequential)      (None, 256)          22043944    ['Anchor_Input[0][0]',           \n",
            "                                                                  'Positive_Input[0][0]',         \n",
            "                                                                  'Negative_Input[0][0]']         \n",
            "                                                                                                  \n",
            " distance_layer (DistanceLayer)  ((None,),           0           ['Encode_Model[3][0]',           \n",
            "                                 (None,))                         'Encode_Model[4][0]',           \n",
            "                                                                  'Encode_Model[5][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 22,043,944\n",
            "Trainable params: 9,583,800\n",
            "Non-trainable params: 12,460,144\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "class DistanceLayer(layers.Layer):\n",
        "    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, anchor, positive, negative):\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
        "        return (ap_distance, an_distance)\n",
        "\n",
        "\n",
        "def get_siamese_network(input_shape = (128, 128, 3)):\n",
        "    encoder = get_encoder(input_shape)\n",
        "\n",
        "    # Input Layers for the images\n",
        "    anchor_input   = layers.Input(input_shape, name=\"Anchor_Input\")\n",
        "    positive_input = layers.Input(input_shape, name=\"Positive_Input\")\n",
        "    negative_input = layers.Input(input_shape, name=\"Negative_Input\")\n",
        "\n",
        "    ## Generate the encodings (feature vectors) for the images\n",
        "    encoded_a = encoder(anchor_input)\n",
        "    encoded_p = encoder(positive_input)\n",
        "    encoded_n = encoder(negative_input)\n",
        "\n",
        "    # A layer to compute ‖f(A) - f(P)‖² and ‖f(A) - f(N)‖²\n",
        "    distances = DistanceLayer()(\n",
        "        encoder(anchor_input),\n",
        "        encoder(positive_input),\n",
        "        encoder(negative_input)\n",
        "    )\n",
        "\n",
        "    # Creating the Model\n",
        "    siamese_network = Model(\n",
        "        inputs  = [anchor_input, positive_input, negative_input],\n",
        "        outputs = distances,\n",
        "        name = \"Siamese_Network\"\n",
        "    )\n",
        "    return siamese_network\n",
        "\n",
        "siamese_network = get_siamese_network()\n",
        "siamese_network.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IeGOdY3EWVYM"
      },
      "outputs": [],
      "source": [
        "class SiameseModel(Model):\n",
        "    # Builds a Siamese model based on a base-model\n",
        "    def __init__(self, siamese_network, margin=1.0):\n",
        "        super(SiameseModel, self).__init__()\n",
        "\n",
        "        self.margin = margin\n",
        "        self.siamese_network = siamese_network\n",
        "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_network(inputs)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # GradientTape get the gradients when we compute loss, and uses them to update the weights\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self._compute_loss(data)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.siamese_network.trainable_weights))\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        loss = self._compute_loss(data)\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def _compute_loss(self, data):\n",
        "        # Get the two distances from the network, then compute the triplet loss\n",
        "        ap_distance, an_distance = self.siamese_network(data)\n",
        "        loss = tf.maximum(ap_distance - an_distance + self.margin, 0.0)\n",
        "        return loss\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We need to list our metrics so the reset_states() can be called automatically.\n",
        "        return [self.loss_tracker]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "tZixI8-cWaPk",
        "outputId": "016c418e-3af4-47da-8887-731a4efe15b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"siamese_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Siamese_Network (Functional  ((None,),                22043944  \n",
            " )                            (None,))                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,043,946\n",
            "Trainable params: 9,583,800\n",
            "Non-trainable params: 12,460,146\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "siamese_model = SiameseModel(siamese_network)\n",
        "siamese_model.built = True\n",
        "siamese_model.load_weights('siamese_model_weights.h5')\n",
        "optimizer = Adam(learning_rate=1e-3, epsilon=1e-01)\n",
        "siamese_model.compile(optimizer=optimizer)\n",
        "\n",
        "siamese_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "libekL_CXLXt",
        "outputId": "730b223d-e2ec-4cea-aa55-c6a586287777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Encode_Model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " xception (Functional)       (None, 2048)              20861480  \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               1049088   \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " lambda_1 (Lambda)           (None, 256)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,043,944\n",
            "Trainable params: 9,583,800\n",
            "Non-trainable params: 12,460,144\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def extract_encoder(model):\n",
        "    encoder = get_encoder((128, 128, 3))\n",
        "    i=0\n",
        "    for e_layer in model.layers[0].layers[3].layers:\n",
        "        layer_weight = e_layer.get_weights()\n",
        "        encoder.layers[i].set_weights(layer_weight)\n",
        "        i+=1\n",
        "    return encoder\n",
        "\n",
        "encoder = extract_encoder(siamese_model)\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tiJLsWbrJgnx"
      },
      "outputs": [],
      "source": [
        "app = Flask(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C1hANSE5XX5R"
      },
      "outputs": [],
      "source": [
        "UPLOAD_FOLDER = 'uploads'\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024/12/31 21:35:14] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\phanh/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\phanh/.paddleocr/whl\\\\rec\\\\latin\\\\latin_PP-OCRv3_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='d:\\\\Anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\dict\\\\latin_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\phanh/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='vi', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
          ]
        }
      ],
      "source": [
        "ocr = PaddleOCR(use_angle_cls=True, lang='vi', rec=True, det=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "mtcnn = MTCNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_mssv_from_image(image_path):\n",
        "    result = ocr.ocr(image_path, cls=True)\n",
        "\n",
        "    if result == [None]:\n",
        "        return \"Normal image\"\n",
        "\n",
        "    detected_text = \"\"\n",
        "    for line in result[0]:\n",
        "        detected_text += line[1][0] + \" \"  \n",
        "\n",
        "    mssv_pattern = r'MSSV:\\s*(\\d+[A-Za-z]+\\d+)'  \n",
        "    match = re.search(mssv_pattern, detected_text)\n",
        "\n",
        "    if match:\n",
        "        return match.group(1)  \n",
        "    else:\n",
        "        return \"MSSV not found\"  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_and_preprocess(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    \n",
        "    faces = mtcnn.detect_faces(img)  \n",
        "    \n",
        "    if len(faces) == 0:\n",
        "        return None, \"No face detected\", None\n",
        "    \n",
        "\n",
        "    x, y, w, h = faces[0]['box']\n",
        "    face = img[int(y):int(y+h), int(x):int(x+w)]\n",
        "\n",
        "    face_resized = cv2.resize(face, (128, 128))\n",
        "    face_normalized = face_resized / 255.0\n",
        "    return np.expand_dims(face_normalized, axis=0), None, face\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Q-o4n4LXXxXI"
      },
      "outputs": [],
      "source": [
        "@app.route('/compare', methods=['POST'])\n",
        "def compare_faces():\n",
        "    if 'image1' not in request.files or 'image2' not in request.files:\n",
        "        return jsonify({'error': 'Both image1 and image2 must be provided.'}), 400\n",
        "\n",
        "    # Save uploaded images\n",
        "    image1 = request.files['image1']\n",
        "    image2 = request.files['image2']\n",
        "\n",
        "    filename1 = secure_filename(image1.filename)\n",
        "    filepath1 = os.path.join(app.config['UPLOAD_FOLDER'], filename1)\n",
        "    image1.save(filepath1)\n",
        "\n",
        "    filename2 = secure_filename(image2.filename)\n",
        "    filepath2 = os.path.join(app.config['UPLOAD_FOLDER'], filename2)\n",
        "    image2.save(filepath2)\n",
        "\n",
        "    img1, error1, cropped_face1 = detect_and_preprocess(filepath1)\n",
        "    img2, error2, cropped_face2 = detect_and_preprocess(filepath2)\n",
        "\n",
        "    sid1 = extract_mssv_from_image(filepath1)\n",
        "    sid2 = extract_mssv_from_image(filepath2)\n",
        "\n",
        "    # Clean up uploaded files\n",
        "    os.remove(filepath1)\n",
        "    os.remove(filepath2)\n",
        "\n",
        "    # Check for detection errors\n",
        "    if error1:\n",
        "        return jsonify({'error': f'Image1: {error1}'}), 400\n",
        "    if error2:\n",
        "        return jsonify({'error': f'Image2: {error2}'}), 400\n",
        "\n",
        "    embedding1 = encoder.predict(img1)\n",
        "    embedding2 = encoder.predict(img2)\n",
        "\n",
        "    threshold = 1.3\n",
        "    distance = np.sum(np.square(embedding1 - embedding2), axis=-1)\n",
        "\n",
        "    confidence = max(0, 100 * (1 - distance / threshold))\n",
        "\n",
        "    prediction = 0 if distance <= threshold else 1\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(cv2.cvtColor(cropped_face1, cv2.COLOR_BGR2RGB))\n",
        "    axes[0].set_title(\"Cropped Image 1\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(cv2.cvtColor(cropped_face2, cv2.COLOR_BGR2RGB))\n",
        "    axes[1].set_title(\"Cropped Image 2\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    return jsonify({\n",
        "        'prediction': int(prediction),\n",
        "        'confidence': round(float(confidence), 2),\n",
        "        'student_id_1': sid1,  \n",
        "        'student_id_2': sid2   \n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "@app.route('/getMSSV', methods=['POST'])\n",
        "def getMSSV():\n",
        "    if 'image1' not in request.files:\n",
        "        return jsonify({'error': 'Image must be provided.'}), 400\n",
        "\n",
        "    # Save uploaded images\n",
        "    image1 = request.files['image1']\n",
        "\n",
        "    filename1 = secure_filename(image1.filename)\n",
        "    filepath1 = os.path.join(app.config['UPLOAD_FOLDER'], filename1)\n",
        "    image1.save(filepath1)\n",
        "\n",
        "    img1, error1, cropped_face1 = detect_and_preprocess(filepath1)\n",
        "\n",
        "    sid1 = extract_mssv_from_image(filepath1)\n",
        "\n",
        "    # Clean up uploaded files\n",
        "    os.remove(filepath1)\n",
        "\n",
        "    # Check for detection errors\n",
        "    if error1:\n",
        "        return jsonify({'error': f'Image1: {error1}'}), 400\n",
        "    \n",
        "    if sid1 == \"MSSV not found\":\n",
        "        return jsonify({'error': f'MSSV not found'}), 400\n",
        "\n",
        "\n",
        "    # axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    # axes[0].imshow(cv2.cvtColor(cropped_face1, cv2.COLOR_BGR2RGB))\n",
        "    # axes[0].set_title(\"Cropped Image 1\")\n",
        "    # axes[0].axis(\"off\")\n",
        "\n",
        "\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "    print('student_id_1'+ sid1)\n",
        "    return jsonify({\n",
        "        'student_id_1': sid1,  \n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "@app.route('/', methods=['GET'])\n",
        "def test():\n",
        "\n",
        "    return jsonify({\n",
        "        'success': True,  \n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G0IQXzwsJkYH",
        "outputId": "d4daad77-a6ee-47a0-b7aa-d8869dedca6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2024-12-31 22:44:16,114] [   ERROR] process.py:99 - t=2024-12-31T22:44:16+0700 lvl=eror msg=\"session closed, starting reconnect loop\" obj=tunnels.session obj=csess id=e9309a6bf105 err=\"read tcp 192.168.8.129:55439->3.20.27.198:443: wsarecv: An established connection was aborted by the software in your host machine.\"\n",
            "[2024-12-31 22:44:16,174] [   ERROR] process.py:99 - t=2024-12-31T22:44:16+0700 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"failed to dial ngrok server with address \\\"connect.us.ngrok-agent.com:443\\\": dial tcp: lookup connect.us.ngrok-agent.com: no such host\"\n",
            "[2024-12-31 22:44:18,740] [   ERROR] process.py:99 - t=2024-12-31T22:44:18+0700 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "[2024-12-31 22:44:20,495] [   ERROR] process.py:99 - t=2024-12-31T22:44:20+0700 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "[2024-12-31 22:44:23,242] [   ERROR] process.py:99 - t=2024-12-31T22:44:23+0700 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "[2024-12-31 22:44:28,015] [   ERROR] process.py:99 - t=2024-12-31T22:44:28+0700 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "[2024-12-31 22:44:32,845] [   ERROR] process.py:99 - t=2024-12-31T22:44:32+0700 lvl=eror msg=\"heartbeat timeout, terminating session\" obj=tunnels.session obj=csess id=cdefca48ff1c clientid=f151a930ce38938a872b8c500dd27cde\n",
            "[2024-12-31 23:08:48,032] [ WARNING] process.py:99 - t=2024-12-31T23:08:48+0700 lvl=warn msg=\"Stopping forwarder\" name=http-5000-3e85394a-cd30-4c4a-bff6-b1ff4178cc6e acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    # Expose the Flask app via ngrok\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f'Flask app running at {public_url}')\n",
        "\n",
        "    # Run the Flask app\n",
        "    app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
